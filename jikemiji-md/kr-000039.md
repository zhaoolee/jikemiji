---
title: Kr039 为了收集文字素材，我完成了《一言》的开源爬虫
---

一言网隶属于萌创团队，目前网站主要提供一句话服务。 

https://hitokoto.cn/


每次打开一言网页，会自动刷出一句话，并注名出处，里面的句子由网友贡献，管理员审核通过后，就会成为对外的素材


zhaoolee作为一个重度收集爱好者，写了一个爬虫，将一言网所有句子收集起来，展示到网页上随时查看

## 代码开源地址

```
https://github.com/zhaoolee/yiyan_spider
```


## 我还简单写了两个配套网页


#### 网页1:适合通过关键词查找句子的网页

在这个网页你可以,在浏览器`ctrl+f`，根据关键词查找句子
```
https://www.v2fy.com/yiyan_spider/yiyan.html
```
![https://www.v2fy.com/yiyan_spider/yiyan.html](https://www.v2fy.com/asset/0i/new_yiyan_qr.png)


#### 网页2: 适合根据分类观看的页面

你可以通过这个网页只展开自己喜欢的分类查看,分类有 **动画**,**漫画**,**游戏**,**文学**,**原创**,**来自网络**,**其他**,**影视**,**诗词**,**网易云**,**哲学**,**抖机灵**

```
https://www.v2fy.com/yiyan_spider/yy.html
```

![https://www.v2fy.com/yiyan_spider/yy.html](https://www.v2fy.com/asset/0i/new_yy_html_qr.png)


## 实现的思路
用nodejs写一个无限循环的爬虫,每隔6到10秒, 按分类去获取一次一言的数据,然后将获得的数据存到本地json文件(新数据使用json自动去重),然后写一个网页获取json文件的数据,展示数据



